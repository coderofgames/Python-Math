{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "   \n",
    "Space Fill: Introductory material\n",
    "\n",
    "Notes from the Stanford Machine learning video series taught by Andrew Ng.\n",
    "   \n",
    "   https://www.youtube.com/watch?v=nLKOQfKLUks&index=4&list=PLA89DCFA6ADACE599\n",
    "\n",
    "   \n",
    "   \n",
    "#### Example 1: regression problem\n",
    "   \n",
    "collect data on housing prices (portland oregan\n",
    "   \n",
    "1. Plot square foot of the house against list price\n",
    "\n",
    "2. Try to predict how much a house will sell for based on size   \n",
    "   Fit a line or a quadratic function to the graph, \n",
    "   \n",
    "This is an example of **Supervised** Learning, because the algorithm is provided with a dataset consisting of\n",
    "floor space in square feet and the **actual price** for each sample (the right answer).\n",
    "   \n",
    "The problem is an example of a **regression problem** since the variable that is being predicted is a continuous value.\n",
    "    \n",
    "> Big question, do you need more data, or do you already have enoug data?\n",
    "   \n",
    "#### Example 2: Classification Problems\n",
    "   \n",
    "The variable to predict is discrete rather than continuous.\n",
    "   \n",
    "Standard dataset on breast cancer tumors. The algorithm predicts whether a tumor is malignent.\n",
    "   \n",
    "1. Collect a set of features, use tumor size as an example\n",
    "   \n",
    "2. The y axis is either malignent or benign, so either 0 or 1\n",
    "   \n",
    "3. There may be more than one **feature**, for example plot age vs tumor size as a scatter plot with crosses    \n",
    "representing malignent tumors and circles representing benign tumors. Put most malignent tumors in one half and    \n",
    "most benign tumors in the other half, with an island, the taks is to make an algorithm to detect the island. This    \n",
    "therefore becomes a **supervised learning** problem and a classification problem.\n",
    "   \n",
    "4. The prediction of benign or malignent is based on two **features** or **inputs**\n",
    "   \n",
    "5. In the real world data is likely to be higher dimensional, even infinite dimensional. (support vector machines->infinite dimensional space)\n",
    "\n",
    "   \n",
    "### Unsupervised Learning\n",
    "   \n",
    "1. Similar to the last example, the plot of age vs tumor size is used, however there aren't any circles and all data   \n",
    "is plotted with crosses. The question is to fund any structure from the data. (add a feature like patient survivability)\n",
    "   \n",
    "2.The algorithm might cluster the data, find that it is partitioned into 2 groups.\n",
    "   \n",
    "This occurs in fields like image processing, genetics, social network analysis, marketing, networking, astromony.\n",
    "   \n",
    "##### Coctail Party Problem\n",
    "   \n",
    "Problem: Room full of people talking, can you separate out the voice of the person you want to hear. \n",
    "   \n",
    "Putting 2 microphones in the room, each microphone will pick up an overlapping contribution from peoples voices, can you separate out the different recordings?\n",
    "   \n",
    "##### Learn to fly a helicopter.\n",
    "    \n",
    "### Lecture 2:\n",
    "   \n",
    "<table>\n",
    "<tr> <td>Space $Ft^2$ </td><td>Number of Bedrooms </td><td>Price 1000 </td></tr>\n",
    "<tr> <td>2104</td><td>3 </td><td>400</td></tr>\n",
    "<tr> <td>1416</td><td>2 </td><td>232</td></tr>\n",
    "<tr> <td>1534</td><td>3 </td><td>315</td></tr>\n",
    "<tr> <td>852</td><td>2 </td><td>178</td></tr>\n",
    "<tr> <td>1940</td><td>4 </td><td>240</td></tr>\n",
    "\n",
    "</table>\n",
    "   \n",
    "Given a training set how do you learn to predict the relationship between the variables?\n",
    "   \n",
    "m = number of training samples   \n",
    "$x = $ input variables   \n",
    "$y = $ output variables (target variable)    \n",
    "   \n",
    "One pair $(x,y)$ or one row is one training sample.   \n",
    "   \n",
    "The $i^th$ training sample (row in the table) is $(x^{(i)}, y^{(i)})$\n",
    "   \n",
    "1. Input the training samples into a learning algorithm\n",
    "   \n",
    "2. The algorithm has a **hypothesis**, $h$, that takes a new input and outputs an estimation for y\n",
    "   \n",
    "### To design a learning algorithm the first thing we want to work out is how to represent the hypothesis.\n",
    "   \n",
    "A linear hypothesis \n",
    "   \n",
    "$h(x) = \\theta_0 + \\theta_1 x$\n",
    "   \n",
    "To name the variables\n",
    "   \n",
    "$ x_1 = $ the size in $ft^2$,    $x_2 = $ the number of bedrooms, then\n",
    "   \n",
    "$h(x) =h_{\\theta}(x)= \\theta_0 + \\theta_1 x_1+\\theta_2 x_2$\n",
    "   \n",
    "Then write $x_0 = 1$ so\n",
    "   \n",
    "$h(x) = \\sum_{i=0}^2 \\theta_i x_i = \\mathbf \\theta^T \\mathbf x$\n",
    "   \n",
    "finally\n",
    "   \n",
    "$n=$ number of features, in this case n=2\n",
    "   \n",
    "The $\\theta_i$ are called the **parameters**, they are just numbers, the learning algorithm must learn the appropriate parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How do we  choose $\\theta$ so that our hypothesis will be correct, we have a training set so our hypothesis will preduict values on the houses, so we need to increase accuracy\n",
    "   \n",
    "$(h(x)-y)^2$ is the squared error on the training set,\n",
    "   \n",
    "we need to minimuze \n",
    "   \n",
    "$$min_\\theta \\frac{1}{2}\\sum_{i=1}^m (h(x)-y)^2$$\n",
    "   \n",
    "Now set $G(\\theta) = \\frac{1}{2}\\sum_{i=1}^m (h(x)-y)^2$ and the task is to minimuze\n",
    "   \n",
    "$$min_\\theta G(\\theta)$$\n",
    "   \n",
    "   > This is a special case of a more general algorithm\n",
    "    \n",
    "   \n",
    "Algorithms for minimization\n",
    "   \n",
    "1. Search algorithm\n",
    "   start with $\\theta$ and say vector $(\\theta = 0)$ and keep changing $\\theta$ to reduce $G(\\theta)$ until a minimum is reached.  \n",
    "   \n",
    "#### Gradient Descent\n",
    "   \n",
    "choose some initial point. Take the gradient of $h(\\mathbf x)$ with respect to $\\mathbf x$ and take a step in the direction of steepest descent. This process repeats until you find a minimum.\n",
    "   \n",
    "> When run from different positions the algorithm can converge to different local minima.\n",
    "   \n",
    "$$\\theta_i := \\theta_i - \\alpha \\frac{\\partial}{\\partial \\theta_i} G(\\theta)$$\n",
    "   \n",
    "The $:=$ operator updates the values of $\\theta_i$ for a consecutive iteration, like saying $\\theta_{i+1}=\\theta_i - \\alpha \\frac{\\partial}{\\partial \\theta_i}G(\\theta)$. Now ...\n",
    "   \n",
    "$$\\frac{\\partial}{\\partial \\theta_i} G(\\theta) = \\frac{\\partial}{\\partial \\theta_i} \\frac{1}{2}(h_\\theta (x) - y)^2 = 2 \\frac{1}{2}(h_\\theta (x) - y)\\cdot \\frac{\\partial}{\\partial \\theta_i}(h_\\theta (x) - y) = (h_\\theta (x)-y) x_i$$\n",
    "   \n",
    "Therefore the learning rule is \n",
    "   \n",
    "$$ \\theta_i := \\theta_i - \\alpha (h_\\theta (x) - y).x_i$$\n",
    "   \n",
    "where $\\alpha$ is called the learning rate, which determins how far in the direction of the gradient each step takes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Gradient Descent\n",
    "   \n",
    "Repeat until convergence\n",
    "   \n",
    "$$ \\theta_i := \\theta_i - \\alpha \\sum_{j=1}^m (h_\\theta(x^{(j)}) - y^{(j)}).x_i^{(j)}$$\n",
    "   \n",
    "for $m$ samples $\\mathbf x^{(j)}$. \n",
    "   \n",
    "For ordinary least squares the function $G$ is usually a qudratic function with one minima. \n",
    "   \n",
    "The step sizes automatically get smaller and smaller as zero is approached. \n",
    "   \n",
    "The algorithm is known as **Batch** gradient descent, using the entire training set. The calculation for all the training samples get summed over, so can take a long time for very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c60400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "x_1 = np.array([2104, 1416, 1534, 852, 1940]) # space\n",
    "x_2 = np.array([3,2,3,2,4]) # num_rooms\n",
    "y = np.array([400,232,315,178,240]) #pirce\n",
    "\n",
    "plt.scatter(x_1, y, c=\"b\")\n",
    "plt.xlabel(\"space\")\n",
    "plt.ylabel(\"price\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDXOoJekxhn0ktQ4g16SGmfQS1Lj/j8KUAShdVdPIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa3a92e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta = np.zeros(2)\n",
    "x_0 = np.array([1,1,1,1,1])\n",
    "\n",
    "\n",
    "h = 0\n",
    "sum = 0\n",
    "end = 0\n",
    "while end <20:\n",
    "    for x0,xj,yj in np.c_[x_0,x_1,y]:\n",
    "\n",
    "        h = theta[0]*x0 + theta[1]*xj\n",
    "    \n",
    "        sum = (h-yj)*x0 + (h-yj)*xj\n",
    "    \n",
    "        thet0 = theta[0] - 0.2 * sum * x0\n",
    "        thet1 = theta[1] - 0.2 * sum * xj\n",
    "        end = end+1\n",
    "        theta[0] = thet0\n",
    "        theta[1] = thet1\n",
    "        \n",
    "## Daves Note: This isn't working, Blame my numpy skills (growing)\n",
    "\n",
    "## so I found this http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/\n",
    "## and modified it to fit the example, see below\n",
    "        \n",
    "plt.scatter(x_1,y)\n",
    "#plt.plot( h)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5L, 2L), (1L, 2L), (5L, 1L))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeCost(X, y, theta):  \n",
    "    inner = np.power(((X * theta.T) - y), 2)\n",
    "    return np.sum(inner) / (2 * len(X))\n",
    "\n",
    "# Code taken from\n",
    "# http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/\n",
    "# Modified by David Nash to fit the text example\n",
    "\n",
    "# append a ones column to the front of the data set (no need)\n",
    "#data.insert(0, 'Ones', 1)\n",
    "\n",
    "# set X (training data) and y (target variable)\n",
    "#cols = data.shape[1]  \n",
    "X = np.c_[x_0,x_1]#data.iloc[:,0:cols-1] \n",
    "\n",
    "## Daves Note: using np.c_ we have concatenated x_0 and x_1 into a 2d array.\n",
    "## Don't need to rewrite y\n",
    "\n",
    "#y = ata.iloc[:,cols-1:cols]  \n",
    "\n",
    "# convert from data frames to numpy matrices\n",
    "X = np.matrix(X)  \n",
    "y = np.matrix(y).T # from inspection of the output below, y has to be transposed.  \n",
    "theta = np.matrix(np.array([0,0]))  \n",
    "X.shape, theta.shape, y.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40233"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, theta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iters):  \n",
    "    temp = np.matrix(np.zeros(theta.shape))\n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    cost = np.zeros(iters)\n",
    "\n",
    "    for i in range(iters):\n",
    "        error = (X * theta.T) - y## Daves Note: The term called error is (h(x)-y)\n",
    "\n",
    "        for j in range(parameters):\n",
    "            term = np.multiply(error, X[:,j])  ## the j^th theta multiplies the j^th x \n",
    "            temp[0,j] = theta[0,j] - ((alpha) * np.sum(term)) # I'm not dividing by m\n",
    "\n",
    "        theta = temp\n",
    "        cost[i] = computeCost(X, y, theta)\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  1.12959358e-04,   1.71691593e-01]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize variables for learning rate and iterations\n",
    "alpha = 0.0000001  \n",
    "iters = 5\n",
    "\n",
    "# Daves Note: alpha has to be very small in this dataset or the solution diverges\n",
    "# Increasing the number of iterations by 1 order of magnitude increases the x_0 by one order\n",
    "# of magnitude, but it doesn't make much difference to the total error\n",
    "\n",
    "# perform gradient descent to \"fit\" the model parameters\n",
    "g, cost = gradientDescent(X, y, theta, alpha, iters)  \n",
    "g  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1396.1383789726999"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, g)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa82e668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Floor Space vs House Price\n",
    "\n",
    "x = np.linspace(0, 3000, 100)  \n",
    "h = g[0, 0] + (g[0, 1] * x)   # Daves Note: renamed this to h=hypothesis \n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(12,8))  \n",
    "plt.plot(x, h, 'r', label='Prediction')  \n",
    "plt.scatter(x_1, y, label='Traning Data')  \n",
    "\n",
    "#Daves Note: Clearly a house with 0 floor space will cost nothing.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2P8D8PjXueepCCcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9835d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Error VS Training Epoch\n",
    "\n",
    "plt.plot(np.arange(iters), cost, 'r')  \n",
    "#ax = plt.axis()  \n",
    "#ax.set_xlabel('Iterations')  \n",
    "#ax.set_ylabel('Cost')  \n",
    "#ax.set_title('Error vs. Training Epoch') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Through the Algorithm\n",
    "   \n",
    "1. Initialize data and variables   \n",
    "x_1 = np.array([2104, 1416, 1534, 852, 1940])    \n",
    "y = np.array([400,232,315,178,240])    \n",
    "x_0 = np.array([1,1,1,1,1])   \n",
    "\n",
    "2. Create the parameters for the hypothesis\n",
    "$ h(x) = \\theta_0 x_0 + \\theta_1 x_1 = \\theta^T \\mathbf x$\n",
    "create the $\\mathbf x$ vector by concatenating $x_0$ and $x_1$.\n",
    "X = np.c_[x_0,x_1]\n",
    "and initialize $\\theta$ to zero\n",
    "theta = np.matrix(np.array([0,0]))  \n",
    "\n",
    "3. Convert the arrays into matrices   \n",
    "X = np.matrix(X)  \n",
    "y = np.matrix(y).T \n",
    "\n",
    "4. Compute Cost function\n",
    "$G(\\theta) = \\frac{1}{2}\\sum_{i=1}^m (h(x)-y)^2 $\n",
    "Summing up all the h(x)-y for each training sample.\n",
    "in this example, the code also divides by the number of parameters \"len(X)\"\n",
    "\n",
    "def computeCost(X, y, theta):  \n",
    "    inner = np.power(((X * theta.T) - y), 2);\n",
    "    return np.sum(inner) / (2 * len(X))\n",
    "    \n",
    "5. Compute Gradient and update each theta\n",
    "    i) compute the term\n",
    "    $ r=(\\theta^T \\mathbf x) -\\mathbf y)$\n",
    "    \n",
    "    ii) For $\\theta_0$ :\n",
    "    $ S = \\sum_{j=1}^m (h_\\theta(x^{(j)}) - y^{(j)}).x_0^{(j)}$ This is the inner product of \n",
    "    $\\theta^T\\mathbf x - \\mathbf y$ and the first column of $\\mathbf X = [\\mathbf x_0, \\mathbf x_1]$.\n",
    "$ \\theta_1 := \\theta_1 - \\alpha S_0 $.\n",
    "\n",
    "    iii) For $\\theta_1$ :\n",
    "    $ S_1 = \\sum_{j=1}^m (h_\\theta(x^{(j)}) - y^{(j)}).x_1^{(j)}$ or the inner product Then\n",
    "    $ \\theta_1 := \\theta_1 - \\alpha S_1 $.\n",
    "\n",
    "6. Repeat until convergence. \n",
    " > The code does not supply the convergence test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the same steps in Sympy (expanded form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}1 & 1 & 1 & 1 & 1\\\\2104 & 1416 & 1534 & 852 & 1940\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡ 1     1     1     1    1  ⎤\n",
       "⎢                           ⎥\n",
       "⎣2104  1416  1534  852  1940⎦"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "init_printing()\n",
    "\n",
    "x_0 = Matrix([1,1,1,1,1])\n",
    "x_1 = Matrix([2104, 1416, 1534, 852, 1940]) # space\n",
    "#x_2 = np.array([3,2,3,2,4]) # num_rooms\n",
    "y = Matrix([400,232,315,178,240]) #pirce\n",
    "\n",
    "X = x_0.col_insert(1,x_1)\n",
    "X = X.T\n",
    "X2 = Matrix([[1,1,1,1,1], [2104, 1416, 1534, 852, 1940]])\n",
    "theta=Matrix([0,0])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "[0  0  0  0  0]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.T*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}-400 & -232 & -315 & -178 & -240\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "[-400  -232  -315  -178  -240]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = theta.T*X-y.T # need to transpose y.\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here clearly the training samples occupy columns, so the first column is the first set of x's in X\n",
    "   \n",
    "See the matrix $\\mathbf X$ as a row of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}-1365\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "[-1365]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r * x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}-2270578\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "[-2270578]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r*x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}-1365 & -2270578\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "[-1365  -2270578]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r*X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}40233.3\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "[40233.3]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division # need this to perform division\n",
    "\n",
    "def cost(r, m):\n",
    "    return r*r.T / (2*m)\n",
    "\n",
    "N(cost(r,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}0.000215897733514349\\\\0.171047560820743\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "⎡0.000215897733514349⎤\n",
       "⎢                    ⎥\n",
       "⎣ 0.171047560820743  ⎦"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(alpha, it, theta, X, y, m):\n",
    "    for i in range(0,it):\n",
    "        theta = (theta.T - alpha*(theta.T*X-y.T)*X.T).T\n",
    "    return theta\n",
    "\n",
    "sol = gradient_descent(0.0000001, 50, theta, X, y, 5)\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDKdELyKS4ZToRUQy3P8DuX5YCe+pLLkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7cb41d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tt = np.linspace(0,3000,100)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "# plotting the points\n",
    "pts = Matrix(([x_1],[y]))\n",
    "for i in range(0,5):\n",
    "    ax.scatter(N(pts[0,0][i]), N(pts[1,0][i]), c='g')\n",
    "ax.plot(tt, N(sol[0]) + N(sol[1])*tt, 'r', label='Prediction')      \n",
    "plt.show()\n",
    "#plt.scatter(x1, y1, label='Traning Data')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient descent\n",
    "   \n",
    "Repeast until convergence:   \n",
    "   For j = 1 to m:  \n",
    "     $\\theta_i:=\\theta_i-\\alpha (h_\\theta(x)^{(j)}-y^{(j)})\\cdot x_i^{(j)}$\n",
    "     for all values of i\n",
    "   \n",
    "Each $j$ is a training sample, each $i$ is a feature.   \n",
    "   For large edatasets, you don't need to look at all samples, only the first set, this will not always converge to the global minimum at first, and the parameters may wander around the global minumum in practice. The algorithm is much faster than batch gradient descent in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "\n",
    "### Matrix notation\n",
    "   \n",
    "$\\nabla G(\\theta) = (\\frac{\\partial }{\\partial \\theta_0},\\cdots,\\frac{\\partial }{\\partial \\theta_i},\\cdots,\\frac{\\partial }{\\partial \\theta_{n}})^T$\n",
    "   \n",
    "here $\\theta$ is an $n+1$ dimensional vector with indicees (0...n). Gradient descent\n",
    "   \n",
    "$$ \\mathbf \\theta:= \\mathbf \\theta - \\alpha \\nabla G$$ \n",
    "   \n",
    "for n+1 dimensional vectors. More generally\n",
    "   \n",
    "$$ f.\\mathbb R^{m\\times n} \\to \\mathbb R $$\n",
    "   \n",
    "the function $f$ maps from the space of $m\\times n$ matrices to the space of real numbers, then\n",
    "   \n",
    "$f(\\mathbf A):A\\in \\mathbb R^{m\\times n}$\n",
    "   \n",
    "then $\\nabla_{\\mathbf A}f(\\mathbf A) $ is the matrix\n",
    "   \n",
    "$$  \\begin{array}{ccc} \\frac{\\partial f}{\\partial A_{11}} & \\cdots & \\frac{\\partial f}{\\partial A_{1m}}\\\\ \\cdots & \\cdots &\\cdots  \\\\ \\frac{\\partial f}{\\partial A_{n1}}&\\cdots &\\frac{\\partial f}{\\partial A_{mn}} \\end{array} $$\n",
    "   \n",
    "If $\\mathbf A \\in \\mathbb R^{n \\times n}$ then\n",
    "   \n",
    "$$ tr \\mathbf A = \\sum_{i=0}^n A_{ii}$$\n",
    "   \n",
    "Fact: \n",
    "   \n",
    "$ tr \\mathbf A \\mathbf B = tr \\mathbf B \\mathbf A$\n",
    "   \n",
    "and\n",
    "   \n",
    "$ tr \\mathbf A \\mathbf B \\mathbf C = tr \\mathbf C \\mathbf A \\mathbf B = tr \\mathbf B \\mathbf C \\mathbf A   $\n",
    "   \n",
    "For a function $f(\\mathbf A) = tr \\mathbf A\\mathbf B $, the derivative $\\nabla_{\\mathbf A}f =\\nabla_{\\mathbf A} tr \\mathbf A\\mathbf B = \\mathbf B^T$.\n",
    "   \n",
    "and \n",
    "   \n",
    "$ tr\\mathbf A = tr \\mathbf A^T%\n",
    "   \n",
    "The trace of a real number is just the number.\n",
    "   \n",
    "$$\\nabla_{\\mathbf A} tr\\mathbf {ABA^TC} = \\mathbf{CAB} + \\mathbf C^T \\mathbf A \\mathbf B^T$$\n",
    "   \n",
    "### Minimizing G in closed form\n",
    "   \n",
    "Create a matrix $X$ of all the inputs ...\n",
    "   \n",
    "$$  \\left(\\begin{array}{ccc} - & \\left(x^{(1)}\\right)^T&-\\\\ - & \\left(x^{(2)}\\right)^T&-\\\\ \\cdots &\\cdots &\\cdots \\\\ -& \\left(x^{(m)}\\right)^T&- \\end{array} \\right)$$\n",
    "   \n",
    "This contains all the inputs of the training set, each row is a training sample, the **design matrix** $X$ has m rows.\n",
    "   \n",
    "$$ \\mathbf X \\mathbf \\theta =  \\left(\\begin{array}{ccc}   \\left(x^{(1)}\\right)^T \\mathbf \\theta \\\\  \\left(x^{(2)}\\right)^T\\mathbf \\theta \\\\ \\cdots \\\\  \\left(x^{(m)}\\right)^T \\mathbf \\theta & \\end{array} \\right)$$\n",
    "   \n",
    "But this is ...\n",
    "   \n",
    "$$  \\left(\\begin{array}{ccc}   h_{\\mathbf \\theta}\\left(x^{(1)}\\right)  \\\\  h_{\\mathbf \\theta}\\left(x^{(2)}\\right) \\\\ \\cdots \\\\  h_{\\mathbf \\theta}\\left(x^{(m)}\\right) \\end{array} \\right)$$\n",
    "   \n",
    "And the outputs for the training set\n",
    "   \n",
    "$$ \\mathbf y=  \\left(\\begin{array}{ccc}  y^{(1)}\\\\ y^{(2)} \\\\ \\cdots \\\\ y^{(m)} \\end{array} \\right)$$\n",
    "   \n",
    "So the $\\mathbf y$ vector is an m dimensional vector. \n",
    "   \n",
    "$ \\mathbf X\\mathbf \\theta - \\mathbf y$ is am m dimensional vector for m training samples.\n",
    "   \n",
    "$$ \\frac{1}{2}( \\mathbf X\\mathbf \\theta - \\mathbf y)^T  \\cdot(\\mathbf X\\mathbf \\theta - \\mathbf y) = \\sum_{i=1}^m (h(x^{(i)}-y^{(i)})^2 = G(\\mathbf \\theta) $$\n",
    "   \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In order to minimuize $G(\\theta)$ we are going to take the derivative, set this to zero and solve for $\\theta$. \n",
    "    \n",
    " $$\\nabla_\\theta G(\\theta) = 0$$\n",
    "    \n",
    "   This is    \n",
    " $$ \\nabla_\\theta (\\frac{1}{2}( \\mathbf X\\mathbf \\theta - \\mathbf y)^T  \\cdot(\\mathbf X\\mathbf \\theta - \\mathbf y)) = \\frac{1}{2} \\nabla_\\theta (\\mathbf \\theta^T \\mathbf X^T \\mathbf X \\mathbf \\theta - \\mathbf \\theta^T \\mathbf X^T y - \\mathbf y^T\\mathbf X\\mathbf \\theta + \\mathbf y^T \\mathbf y)$$\n",
    "    \n",
    " Since the returned value of the term in parentheses is just a number, this is also equal to \n",
    "   \n",
    "$$\\frac{1}{2} \\nabla_\\theta tr(\\mathbf \\theta^T \\mathbf X^T \\mathbf X \\mathbf \\theta - \\mathbf \\theta^T \\mathbf X^T y - \\mathbf y^T\\mathbf X\\mathbf \\theta + \\mathbf y^T \\mathbf y)$$\n",
    "   \n",
    "and applying cyclic permutation to the first term \n",
    "   \n",
    "$$\\frac{1}{2}\\left( \\nabla_\\theta tr(\\mathbf \\theta\\mathbf \\theta^T \\mathbf X^T \\mathbf X)  -\\nabla_\\theta tr( \\mathbf \\theta^T \\mathbf X^T y) - \\nabla_\\theta tr(\\mathbf y^T\\mathbf X\\mathbf \\theta) + \\nabla_\\theta tr(\\mathbf y^T \\mathbf y)\\right)$$\n",
    "   \n",
    "The transpose of a real number is just the real number again, applying this to the second and third terms with cyclic permutation\n",
    "   \n",
    "$$\\frac{1}{2}\\left( \\nabla_\\theta tr(\\mathbf \\theta\\mathbf \\theta^T \\mathbf X^T \\mathbf X)  -\\nabla_\\theta tr( \\mathbf y^T \\mathbf X \\mathbf \\theta) - \\nabla_\\theta tr(\\mathbf y^T\\mathbf X\\mathbf \\theta)\\right)$$\n",
    "   \n",
    "   since the last term does not depend on theta\n",
    "From the identity above,\n",
    "   \n",
    "$$ \\nabla_\\theta tr (\\theta \\mathbf I \\mathbf \\theta ^2 \\mathbf X^2 \\mathbf X) = \\mathbf X^T\\mathbf X \\mathbf \\theta \\mathbf I + \\mathbf X^T \\mathbf X \\mathbf \\theta \\mathbf I$$\n",
    "   \n",
    "and \n",
    "   \n",
    "$$ \\nabla_\\theta tr\\mathbf y^T\\mathbf X\\mathbf \\theta = \\mathbf X^T \\mathbf y$$\n",
    "   \n",
    "and so \n",
    "   \n",
    "$$\\frac{1}{2}\\left( \\mathbf X^T\\mathbf X \\mathbf \\theta + \\mathbf X^T \\mathbf X \\mathbf \\theta  -\\mathbf X^T \\mathbf y -\\mathbf X^T \\mathbf y\\right)= \\mathbf X^T\\mathbf X \\mathbf \\theta  -\\mathbf X^T \\mathbf y$$\n",
    "   \n",
    "These are called the **normal equations** and they are set equal to zero and solved for $\\theta$ and gives us a way of solving for the least squares best fit of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 3\n",
    "   \n",
    "- Adaptation of linear regression - **locally weighted regression**. \n",
    "\n",
    "- Probabalistic Interpretation of linear regression\n",
    "   \n",
    "- Neural Networks, Perceptron algorithm.\n",
    "- Newtons Method.\n",
    "   \n",
    "> Brief summary of last lecture.\n",
    "   \n",
    "We used two features last time, e.g. house size, number of bedrooms. The choice of features will often have a large impact on what it actually does.\n",
    "   \n",
    "> it is possible to use two features like the size of the house and (size)^2 of the house, then the algorithm will end up fitting a quadratic function ...\n",
    "   \n",
    "$$ h(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2$$\n",
    "   \n",
    "If you fit a higher order polynomial, you can fit each of the points exactly. This method can tend to **over fit** the data.\n",
    "   \n",
    "> back to this later. There are **feature selection algorithms**.\n",
    "\n",
    "### Non-parametric Learning algorithms\n",
    "   \n",
    "Linear regression is an example of a parametric learning algorithm, it has a fixed number of parameters $\\theta$ that fit to the data.\n",
    "   \n",
    "> A non-parametric learning algorithm is an algorithm where the number of parameters grows linearly with m (the size of the training set)\n",
    "   \n",
    "A less formal definition is that the amount of stuff that the algorithm keeps is proportional to the size of the training set.\n",
    "   \n",
    "#### Locally Weighted Regression (Lowess)\n",
    "   \n",
    "= tale a traomomg set that looks like a clear curve function (rise time with overshoot low stead state)\n",
    "   \n",
    "Evaluate $h$ at a certain position $x$. The algorithm fits data only in the vicinity of $x$ and linear regression is applied to the subset of the data near the required point.\n",
    "    \n",
    "LWR : Fit $\\theta$ to minimize:\n",
    "$$\\sum_i w^{(i)}(y^{(i)}-\\theta^T x^{(i)} ) ^2$$\n",
    "   \n",
    "where the weights $w^{(i)} = exp\\left(-\\frac{(y^{(i)-x^{(i)})^2}}{2}\\right)$ for example,\n",
    "   \n",
    "If $|x^{(i)} - x| $ is small, then $w^{(i)} \\approx 1$. Conversely\n",
    "   \n",
    "If $|x^{(i)} - x| $ is large, then $w^{(i)} \\approx 0$.    \n",
    "   \n",
    "\n",
    "   The points close to the region of interest are given a large weight. \n",
    "   \n",
    "The $w$ function happens to resemble a Gaussian distribution - any function can be chosen but an exponential decay is appropriate.\n",
    "   \n",
    "Normally there is a parameter $\\tau$ called the **bandwidth paraneter**\n",
    "   \n",
    " $$w^{(i)} = exp\\left(-\\frac{(y^{(i)-x^{(i)})^2}}{2 \\tau}\\right)$$\n",
    "    \n",
    " then $\\tau$ controls how fast the weights fall off. If $\\tau$ is small, the bandwidth is narrow, if $\\tau$ is large, then you have a large bandwidth (or a slow falloff).\n",
    "    \n",
    "> This algorithm fits a locally straight line for every query. \n",
    "   \n",
    "> This algorithm can also suffer from problems like over fitting and under fitting.\n",
    "   \n",
    "### Probabalistic Interpretation of Linear Regression.\n",
    "   \n",
    "how is the least squares interpretation arrived at?\n",
    "   \n",
    "Assume: $y^{(1)} = \\theta^Tx^{(i)} + \\epsilon^{(i)}$, where $\\epsilon^{(i)}$ is the error term that captures other features (forgotton) and random noise (house seller bad mood). \n",
    "   \n",
    "Assume: $\\epsilon^{(i)}$ is $\\mathscr N(0,\\sigma^2) $ is a gaussian distribution or probability distribution, with mean 0 and covariance $\\sigma^2$. The density for $\\epsilon^{(i)}$ is\n",
    "   \n",
    "$$P(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{\\left(\\epsilon^{(i)}\\right)^2}{2\\sigma^2})$$\n",
    "   \n",
    "The density of $\\epsilon^{(i)}$ will be the bell shaped curve with 1 standard deviation being $\\sigma$. \n",
    "   \n",
    "This implies that the probablility distribution of the price of a house \n",
    "   \n",
    "$$ P(y^{(i)}|x^{(i)};\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{\\left(y^{(i)}- \\theta^T x^{(i)}\\right)^2}{2\\sigma^2})$$\n",
    "\n",
    "> The $P(y^{(i)}|x^{(i)};\\theta)$ is the Probability of $y^{(i)}$ given $x^{(i)}$ and $\\theta$. This notation is used when $\\theta$ is not considered to be a random variable. Called the frequentists point of view, some value of $\\theta$ is generating the values. (not Bayesian viewpont)\n",
    "   \n",
    "Or the price of a house given $y^{(i)}|x^{(i)};\\theta = \\mathscr N(\\theta^T x^{(i)},\\sigma^2)$. \n",
    "   \n",
    "> Central limit theorem : The sum of many independant random variables will tend towards the Gaussian.\n",
    "   \n",
    "In practice this is a reasonably accurate assumption, and mathematically convenient. \n",
    "   \n",
    "Assume: $\\epsilon^{(i)}$ The error terms are Independantly and Identically Distributed (IID). They come from the same Gaussian distribution, the same variance, and are independant of each other.\n",
    "   \n",
    "$$L(\\theta)=P(\\mathbf y|\\mathbf x;\\theta) = \\prod_{i=1}^m P(y^{(i)}|x^{(i)};\\theta) = \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\left(y^{(i)}- \\theta^T x^{(i)}\\right)^2}{2\\sigma^2}\\right)$$ \n",
    "   \n",
    "> The likelihood of a paramater is noted as the probability function with $x$ and $y$ fixed.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you estimate the parameters?\n",
    "   \n",
    "The principle of maximum likelihood estimation means that $\\theta$ is chosen to maximize $L(\\theta)=P(\\mathbf x|\\mathbf y;\\theta)$. \n",
    "   \n",
    "$\\mathscr l(\\theta)= \\text {log} L(\\theta) $\n",
    "   \n",
    "This is of course\n",
    "   \n",
    "\n",
    "$$\\text{log}\\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\left(y^{(i)}- \\theta^T x^{(i)}\\right)^2}{2\\sigma^2}\\right)$$ \n",
    "   \n",
    "The log of a product is the same as the sum of the logs,\n",
    "   \n",
    "$$ = \\sum_{i=1}^m \\text{log}(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\left(y^{(i)}- \\theta^T x^{(i)}\\right)^2}{2\\sigma^2}\\right))$$\n",
    "   \n",
    "The log and exp cancel, so \n",
    "   \n",
    "$$ = m \\text{log}(\\frac{1}{\\sqrt{2\\pi}\\sigma})+ \\sum_{i=1}^m\\left(-\\frac{\\left(y^{(i)}- \\theta^T x^{(i)}\\right)^2}{2\\sigma^2}\\right)$$\n",
    "   \n",
    "so maximizing the log liklihood is the same as minimizing $\\sum_{i=1}^m\\left(-\\frac{\\left(y^{(i)}- \\theta^T x^{(i)}\\right)^2}{2\\sigma^2}\\right)$ as a function of $\\theta$ and this is the same quadratic cost function $G(\\theta)$. \n",
    "   \n",
    "> Therefore Ordinary least squared is the same as asusming Gaussian IID errors on the data. The value of $\\sigma^2$ does not really matter, the same value of $\\theta$ is arrived on.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Problems\n",
    "   \n",
    "The value $y\\in (0,1)$ is discrete, can either be \"yes\" or \"no\", for example, classify whether the house will sell in 6 months. \n",
    "   \n",
    "- create a dataset that plots first 4 points at zero and second 4 points at 1 (or some discrete value).\n",
    "   \n",
    "> obviously fitting a straight line does not really represent the data\n",
    "\n",
    "If the training set changes and 1 more training example appears with the value 1 at twice the distance of the original final data point, then the curve does not fit, and another linear regreassion makes the predictions of the hypothesis completely different.\n",
    "   \n",
    "> add more points accross to the second point.\n",
    "   \n",
    "If $y\\in[0,1]$ then change the hypothesis\n",
    "$h(x)\\in [0,1]$ ... e.g\n",
    "   \n",
    "$$h_\\theta (x) = g(\\theta^T x) = \\frac{1}{1+e^{-\\theta^T x}}$$\n",
    "   \n",
    "The function $g(z)$ is called the **sigmoid function**, or the **logistic function**.\n",
    "   \n",
    "Plot g(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(z)$ tends to zero as z becomes small, and will asymptote towards 1 as z becomes large.\n",
    "   \n",
    "Assume: that    \n",
    "$$P(y=1|x;\\theta)=h_\\theta (x))$$   \n",
    "the probability that $y-1$ given x and paratermized by $\\theta$ the hypothesis will try to estimate the probability that $y=1$. \n",
    "   \n",
    "$P(y=0|x;\\theta)=1-h_\\theta (x)$\n",
    "   \n",
    "more compactly\n",
    "   \n",
    "$$P(y|x;\\theta) = h_\\theta (x)^y (1-h_\\theta(x))^{(1-y)}$$\n",
    "   \n",
    "since if $y=1$ we get the first equation\n",
    "\n",
    "$$P(y|x;\\theta) = h_\\theta (x)^1 (1-h_\\theta(x))^{(0)}= h_\\theta (x)$$\n",
    "   \n",
    "since if $y=0$ we get the second equation\n",
    "\n",
    " $$P(y|x;\\theta) = h_\\theta (x)^0 (1-h_\\theta(x))^{(1)}= (1) (1-h_\\theta(x))=1-h_\\theta(x)$$ \n",
    "    \n",
    "Given this model, how do we fit $\\theta$. The likliehood is \n",
    "   \n",
    "$$L(\\theta) = P(\\mathbf y\\ | \\mathbf x; \\theta) = \\prod_i P(y^{(i)}\\ |x^{(i)}; \\theta) = \\prod_i h_\\theta (x^{(i)})^{y^{(i)}} (1-h_\\theta(x^{(i)}))^{(1-y^{(i)})} $$\n",
    "   \n",
    "> Now we need a maximum likelihood estimation, a function that maximizes $L(\\theta)$.\n",
    "   \n",
    " $$\\mathscr l(\\theta) = \\text{log} L(\\theta) = \\sum_{i=1}^m y^{(i)}\\text{log} h_\\theta (x^{(i)})+ (1-y^{(i)})\\text{log}(1-h_\\theta (x^{(i)}))$$\n",
    "    \n",
    "An example of a method to maximize this function (this is the **objective function**) is the Gradient Descent method.\n",
    "   \n",
    "$$ \\theta:= \\theta+\\alpha \\nabla_\\theta \\mathscr l(\\theta) $$\n",
    "   \n",
    "Now to find the derivative\n",
    "   \n",
    "$$\\frac{\\partial }{\\partial \\theta_j} \\mathscr l(\\theta) = \\sum_{i=1}^m \\left(y^{(i)} - h_\\theta (x^{(i)}) \\right)\\cdot x_j^{(i)}$$\n",
    "   \n",
    "Therefore **gradient ascent** (in this case, since we are maximizing) is\n",
    "   \n",
    "$$\\theta_j := \\theta_j + \\alpha \\sum_{i=1}^m \\left(y^{(i)} - h_\\theta (x^{(i)}) \\right)\\cdot x_j^{(i)}$$\n",
    "   \n",
    "This is exactrly the same as the formula for batch gradient descent, except that there is a positive sign. \n",
    "   \n",
    "This is not the same, because the definition for $h_\\theta$ is different from the form used in Logistic regression. \n",
    "   \n",
    "> Perceptron's use g(z) = 1 if z= 0, and g(z) = 0 otherwise. (step function). Then $h(x) = g(\\theta^T x)$ and the learning rule is the same as stochastic gradient descent $\\theta_j:=\\theta_j +\\alpha(y^{(i)}-h_\\theta(x^{(i)}))x_j^{(i)}$. This is a very different algorithm from logistic regression. \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 4\n",
    "   \n",
    "- Logistic Regression\n",
    "- Newtons Method\n",
    "- Exponential Family\n",
    "- Generalized Linaer Models\n",
    "   \n",
    "\n",
    "   \n",
    "### Newtons Method\n",
    "   \n",
    "To fit a logistic regression method, this method often runs much faster than gradient descent.\n",
    "   \n",
    "for a function $f(\\theta)$ and you want to find a value of $\\theta$ such that $f(\\theta)=0$.\n",
    "   \n",
    "for example plot a wavy line near y=x. ($\\theta$ horizontal axis).\n",
    "   \n",
    "choose a value of $\\theta = \\theta^{(0)} $ and evaluate $f(\\theta{^(0)})$ then take the tangent of the function $f'(\\theta^{(0)})$, take the tangent down to zero (intersect with the $\\theta$ axis) and name the point $\\theta^{(1)}$, then repeat the process.\n",
    "   \n",
    "The difference $\\theta^{(0)}-\\theta^{(1)} = \\Delta$. \n",
    "   \n",
    "then $f'(\\theta^{(0)}) = \\frac{f(\\theta^{(0)})}{\\Delta} $\n",
    "and so\n",
    "$\\Delta = \\frac{f(\\theta^{(0)})}{f'(\\theta^{(0)})}$\n",
    "   \n",
    "and then $\\theta^{(1)} =\\theta^{(0)}- \\Delta = \\theta^{(0)}-\\frac{f(\\theta^{(0)})}{f'(\\theta^{(0)})} $\n",
    "   \n",
    "and so we obtain the general algorithm\n",
    "   \n",
    "$$\\theta^{(i+1)} =\\theta^{(i)}- \\Delta = \\theta^{(i)}-\\frac{f(\\theta^{(i)})}{f'(\\theta^{(i)})} $$\n",
    "   \n",
    "> This idea can be applied to maximizing the log liklihood.\n",
    "   \n",
    "$\\mathscr l(\\theta) $ want $\\theta : \\mathscr l(\\theta)=0$.\n",
    "   \n",
    "$$\\theta^{(t+1)}=\\theta^{(t)} - \\frac{\\mathscr l'(\\theta^{(t)})}{\\mathscr l(\\theta^{(t)})}$$\n",
    "   \n",
    "Then this must be a local optimimum. (can usually choose $\\mathbf \\theta_0 = \\mathbf 0$.)\n",
    "   \n",
    "> Newtowns method has quadraticly convergent.(every iteration will double the number of significant digits, squares error every iteration).\n",
    "   \n",
    "#### Generalization\n",
    "   \n",
    "$$ \\mathbf \\theta^{(t+1)} = \\mathbf \\theta^{(t)}-\\mathbf H^{-1}\\nabla \\mathscr l(\\mathbf \\theta)$$\n",
    "   \n",
    "where $\\mathbf H$ is the Hessian (derivative of the gradient)\n",
    "   \n",
    "$$H_{ij} = \\frac{\\partial^2 \\mathscr l}{\\partial \\theta_i \\partial \\theta_j}$$\n",
    "   \n",
    "> Disadvantage is the inversion of the Hessian every iteration for large number of features. ($H\\in \\mathbb R^{n\\times n}$ ).\n",
    "   \n",
    "> How does the algorithm change if you want to use it for minimization. (answer it does not change).\n",
    "   \n",
    "### So far...\n",
    "   \n",
    "2 different algorithms for minimizing $P(y|x;\\theta)$. One was a real number\n",
    "   \n",
    "$y \\in \\mathbb R$ assumed to have a Gaussian Distribution, ordinary least squares.\n",
    "   \n",
    "For classification $y\\in[0,1]$ Bernoulli $\\to$ Logistic regression.\n",
    "   \n",
    "### Where did the sigmoid function come from?\n",
    "   \n",
    "Take both algorithms and show that they are part of a generalized set of inear algorithms.\n",
    "   \n",
    "### Bernoulli and Gaussian Distributions\n",
    "   \n",
    "$B(\\phi)$ is the Bernoulli distribution.\n",
    "   \n",
    "$$P(y=1\\ |x;\\phi) = \\phi$$\n",
    "   \n",
    "> The parameter $\\phi$ specifies that probability that $y=1$. As you vary the value of $\\phi$ you get different distributions on y, that have different probabilities of being 1, so this is a set or class of distributions,\n",
    "   \n",
    "$$\\mathscr N(\\mu,\\sigma^2)$$ as you vary $\\mu$ you get other distributions, so is also a class.\n",
    "   \n",
    "> They are both classes of distributions in the exponetial family. \n",
    "   \n",
    "A distribution is in the exponential family if it can be written\n",
    "   \n",
    "$$ P(y;\\eta) = b(y) exp(\\eta^T T(y)-a(\\eta))$$\n",
    "   \n",
    "> here $\\eta$ takes the role of $\\theta$. \n",
    "   \n",
    "$\\eta$ is called the natural parameter of the distribution. (in many cases a real number)\n",
    "$T(y)$ is called the sufficient statistic. (often T(y)=y).\n",
    "   \n",
    "For a given choice of these functions, $a,b,T$ then this formula defines a set / class of distributions parameterized by $\\eta$. The Bernoulli and the Gaussian are special cases,depending on $a,b,T$.\n",
    "   \n",
    "#### Bernoulli\n",
    "   \n",
    "$B(\\phi)$ and $P(y=1\\ |x;\\phi) = \\phi$\n",
    "   \n",
    "$P(y=1\\ |x;\\phi) = \\phi^y(1-\\phi)^{1-y}$\n",
    "   \n",
    "then\n",
    "   \n",
    "$exp(log(\\phi^y(1-\\phi)^{1-y})) = exp(y log (\\phi) + (1-y)log(1-\\phi)) = exp(log{\\frac{\\phi}{1-\\phi}}y + log(1-\\phi))$\n",
    "   \n",
    "The term $log{\\frac{\\phi}{1-\\phi}}= \\eta$, the term $y=T(y)$ and $log(1-\\phi))=-a(\\eta)$.\n",
    "   \n",
    "Solving for $\\eta(\\phi)$ then\n",
    "   \n",
    "$\\phi = \\frac{1}{1-e^{-\\eta}}$ (similar to the logistic function).\n",
    "   \n",
    "then $a(\\eta) = log(1-\\phi) = log(1+e^\\eta)$\n",
    "  \n",
    "and T(y)=y and b(y)=1. \n",
    "   \n",
    "> therefore the Bernoulli distribution fits into the exponential family of distributions.\n",
    "   \n",
    "### Gaussian\n",
    "   \n",
    "Since $\\sigma^2$ does not matter, set it =1.   \n",
    "\n",
    "$$\\mathscr N(\\mu,\\sigma^2)   = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}(y-\\mu)^2)=\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}y^2)exp(\\mu y -\\frac{1}{2}\\mu^2)$$\n",
    "   \n",
    "so $b(y) = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{1}{2}y^2)$ and $T(y)=y$ and $\\mu = \\eta$, \n",
    "Then $a(\\mu) = -\\frac{1}{2}\\mu^2 = -\\frac{1}{2}\\eta^2$. \n",
    "   \n",
    "> Therefore Gaussian distribution is a class of distributions in the exponential family of distributions.\n",
    "   \n",
    "> Most textbook distribs can be written this way\n",
    "   \n",
    "- the multivariate normal distribution (generalization ) is also in the exponential family. (outcomes over k possible values)\n",
    "- the multinomial distribution models multiple outcomes, rather than a coin tos, is also in the exponential family.\n",
    "- Poisson distribution is also in the exponential family. USed for modelling counts. (customers to website etc).\n",
    "- Gamma and Exponential distributions, used over positive numbers, for intervals (when will the next bus arrive, etc). \n",
    "- Beta amd Dirichlet, over fractions, probability distributions over probability distributions.\n",
    "- Wishhar distribution over covariance matrices.\n",
    "\n",
    "### How to derive a generalized linear model.\n",
    "   \n",
    "Assume / Deisgn choices: \n",
    "1. Given $y|x;\\theta$, assume $y$ is distributed ExponentialFamily($\\eta$). \n",
    "2. Given $x$ the goal is to output $E(T(y)|x)$ the expected value of $T(y)$ given $x$.  want $h(x) = E[T(y)|x]$.\n",
    "3. (design choice) relationship betweem $\\theta$ and $\\eta$ assumed linear, $\\eta = \\theta^T x$. for the more general case \n",
    "   \n",
    "### Example.\n",
    "   \n",
    "Bernoulli example\n",
    "   \n",
    "$$y|x;\\theta = ExpFamily(\\eta)$$\n",
    "   \n",
    "Using previous work. For a fixed $x,\\theta$ the algorithm is\n",
    "   \n",
    "$$h(x) = E[y|x;\\theta] = P(y=1 |x, \\theta) = \\phi = \\frac{1}{1-e^{-\\eta}} =\\frac{1}{1-e^{-\\theta^T x}}$$ \n",
    "   \n",
    "The model is automatically derived from the decesion on the distribution.\n",
    "   \n",
    "$g(\\eta) = E[y;\\eta]=\\frac{1}{1-e^{-\\eta}}$ os called the **canonical response function**\n",
    "a $g(\\eta)^{=1}$ is called the **canonical link function**. (some texts use the inverse)\n",
    "   \n",
    "> Make a table of canonical response functions and link functions.\n",
    "   \n",
    "### do the gaussian example.\n",
    "   \n",
    "### More complex example\n",
    "   \n",
    "Multinomial distirbution $y\\in [1,\\cdots, k]$. (predict if a patient has one of k diseases). Multi-class classification probklem.\n",
    "   \n",
    "> Draw a diagram with crosses, circles, triangles, 3 or more classes of data point.\n",
    "   \n",
    "Parameters : $\\phi_1, \\phi_2, \\cdots, \\phi_k$.\n",
    "   \n",
    "$$P(y=i)=\\phi_i$$\n",
    "   \n",
    "Then \n",
    "   \n",
    "$$ \\phi_k = 1-(\\phi_1 + \\cdots + \\phi_{k-1})$$\n",
    "   \n",
    "Parameters: $\\phi_1, \\cdots, \\phi_{k-1}$\n",
    "   \n",
    "in this example, $y=1,\\cdots, k$ Then for $T(y)$ we have vectors ...\n",
    "   \n",
    "$$T(1) = \\left(\\begin{array}{ccc}1\\\\0\\\\...\\\\0\\end{array}\\right) $$\n",
    "   \n",
    "and \n",
    "   \n",
    "$$T(2) = \\left(\\begin{array}{ccc}0\\\\1\\\\...\\\\0\\end{array}\\right) $$\n",
    "   \n",
    "Then\n",
    "   \n",
    "$$T(k-1) = \\left(\\begin{array}{ccc}0\\\\0\\\\...\\\\1\\end{array}\\right) $$\n",
    "\n",
    "and $T(k) = \\mathbf 0$, the vector of all zeros. The indicator function is $1\\{true\\} = 1$ and $1\\{false\\} = 0$. Then $1\\{2=3\\} = 0$ etc\n",
    "   \n",
    "Then \n",
    "   \n",
    "$$ \\mathbf T(\\mathbf y)_i = 1\\{y=i\\}$$\n",
    "   \n",
    "The multinomial distribution is\n",
    "   \n",
    "$$P(y) = \\phi_1^{1\\{y=1\\}}\\phi_2^{1\\{y=2\\}}\\cdots \\phi_k^{1\\{y=k\\}} = \\phi_1^{T(y)_1}\\phi_2^{T(y)_2}\\cdots\\phi_k^{T(y)_k}$$\n",
    "   \n",
    "This can be simplified to the exponential family form where \n",
    "   \n",
    "$$ \\eta = \\left(\\begin{array}{ccc}\\text{log}(\\phi_1/\\phi_k) \\\\ \\cdots \\\\\\text{log}(\\phi_{k-1}/\\phi_k) \\end{array}\\right) \\in \\mathbb R^{k-1}$$\n",
    "   \n",
    "and \n",
    "   \n",
    "$$a(\\eta) = -\\text{log}(\\phi_k), b(y)=1$$\n",
    "   \n",
    "Then you can write $\\phi$ as a function of $\\eta$ through matrix inversion\n",
    "   \n",
    "$$ \\phi_i = \\frac{e^{\\eta_i}}{1+ \\sum_{j=1}^{k-1} e^{\\eta_j}}$$\n",
    "   \n",
    "if $\\eta$ are a linear function\n",
    "   \n",
    "$$ \\phi_i = \\frac{e^{\\theta_i^T x}}{1+ \\sum_{j=1}^{k-1} e^{\\theta_j^T x}}$$\n",
    "   \n",
    "so the learning algorithm is\n",
    "   \n",
    "$$ h(x) = E[\\mathbf T(\\mathbf y)|x;\\theta] = \\mathbf \\phi$$\n",
    "   \n",
    "and T(y) is a vector of indicator functions. So the probability of $T(1)|x;\\theta = \\phi_1$.\n",
    "\n",
    "and $\\phi_1 = \\frac{e^{\\theta_1^T x}}{1+ \\sum_{j=1}^{k-1} e^{\\theta_j^T x}}$ etc.\n",
    "\n",
    "   \n",
    "> This algorithm is called Softmax regression, and is widely thought of as logistic regression to the case of k classes rather than 2 classes.\n",
    "   \n",
    "If $y\\in \\{1,\\cdots,k\\}$ then you say you want a multinomial distribution (exponential family). Everything else follows automatically. Then the training set\n",
    "   \n",
    "$(x^{(i)},y^{(i)}), \\cdots (x^{(m)},y^{(m)})$\n",
    "   \n",
    "The the hypothesis is \n",
    "   \n",
    "$$h(x) = \\prod_{i=1}^m P(y^{(i)}|x^{(i)};\\theta) = \\prod_{i=1}^m \\phi_1^{1\\{y^{(i)}=1\\}}\\phi_2^{1\\{y^{(i)}=2\\}}\\cdots \\phi_k^{1\\{y^{(i)}=k\\}}$$\n",
    "   \n",
    "where for example, \n",
    "      \n",
    "$$\\phi_1 = \\frac{e^{\\theta_1^T x}}{1+ \\sum_{j=1}^{k-1} e^{\\theta_j^T x}}$$\n",
    "   \n",
    "Write this down, take logs, compute the derivative with respect to theta, then apply gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
