{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "---\n",
    "\n",
    "> a point in N-dimensional space is represented by $\\mathbf x = (x^1,x^2,\\cdots,x^n)$\n",
    "> the coordnate system is represented by some basis $(\\mathbf e_1, \\mathbf e_2, \\cdots, \\mathbf e_n)$\n",
    "> the point may be represented in more than one basis (see curvilinear coordinates). \n",
    "\n",
    "Note: a superscript is used for the point vector instead of a subscript. If a component is raised to a power, then it is placed in round brackets $(x^i)^2$ is the $i^{th}$ component of $x$ raised to the power of 2.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Coordinate Transformations\n",
    "   \n",
    "if $ x^1, x^2 \\cdots,x^n$ and $ \\bar{x}^1, \\bar{x}^2,\\cdots, \\bar{x}^n$ are representations of a point in two coordinate systems or frames of reference, and \n",
    "   \n",
    "$$\\bar{x}^1 =\\bar{x}^1( x^1, x^2 \\cdots,x^n)$$\n",
    "$$\\bar{x}^2 =\\bar{x}^2( x^1, x^2 \\cdots,x^n)$$\n",
    "$$\\cdots$$\n",
    "$$\\bar{x}^n =\\bar{x}^n( x^1, x^2 \\cdots,x^n)$$\n",
    "\n",
    "defines the representation of the *barred coordinates* as a function of the *unbarred coordinates*, and to define the converse\n",
    "\n",
    "$$x^k = x_k(\\bar{x}^1, \\bar{x}^2,\\cdots, \\bar{x}^n ) \\qquad k=1,\\cdots,n $$\n",
    "\n",
    "Then these relations define a coordinate transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstein Summation Convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Sums\n",
    "\n",
    "expressions of the form \n",
    "\n",
    "$$\\large\\sum_{i=1}^n x_i = x_1 + x_2 +\\cdots x_n $$\n",
    "\n",
    "are written simply as \n",
    "\n",
    "$$x_i,\\quad i=1,n$$\n",
    "\n",
    "For an expression of the form \n",
    "\n",
    "$$ y_i = a_{ij}x_j $$\n",
    "\n",
    "with $j = 1,2,\\cdots,n$, has two indices. The variable $j$ is called a dummy index, and $i$ is called the free index.\n",
    "\n",
    "$$ y_i = a_{i1}x_1 + a_{i2}x_2 + \\cdots +a_{in}x_n$$\n",
    "\n",
    "Now iterating over the free index to $m$ gives \n",
    "\n",
    "$$ y_1 = a_{11}x_1 + a_{12}x_2 + \\cdots +a_{1n}x_n$$\n",
    "$$ y_2 = a_{21}x_1 + a_{22}x_2 + \\cdots +a_{2n}x_n$$\n",
    "$$ \\cdots $$\n",
    "$$ y_m = a_{m1}x_1 + a_{m2}x_2 + \\cdots +a_{mn}x_n$$\n",
    "\n",
    "This does yeild a matrix definition of the form $ \\mathbf y_{m1} = \\mathbf a_{mn} \\mathbf x_{n1} $ \n",
    "\n",
    "> #### The Summation Convention\n",
    "> Any expression with a twice repeated index (either subscripts (and) or superscripts) represents its sum over the \n",
    "> values $1,2,\\cdots,n$ of the repeated index. The character $n$ may represent the range of summations unless stated \n",
    "> otherwise.\n",
    "   \n",
    "> 1) Any free index shall have the same range as summation indexes unless stated otherwise (in the equation above, n=m by default).\n",
    "   \n",
    "> 2) No index may occur more than twice in an expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple summation indexes\n",
    "\n",
    "An expression with multiple summation indexes represents the result of the sum over each index, so \n",
    "\n",
    "$$\\large a_{\\color{red}{i}\\color{blue}{j}}x_{\\color{red}{i}}y_{\\color{blue}{j}}$$ \n",
    "\n",
    "results in summing over ${\\color{red}{i}}$ from ${\\color{red}{(1,\\cdots, n)}}$ and ${\\color{blue}{j}}$ from ${\\color{blue}{(1,\\cdots, m)}}$, i.e.first summing over $i$ \n",
    "\n",
    "$$a_{\\color{red}{1}\\color{blue}{j}} x_{\\color{red}{1}} y_{\\color{blue}{j}} + \\cdots + a_{\\color{red}{n}\\color{blue}{j}} x_{\\color{red}{n}} y_{\\color{blue}{j}} $$\n",
    "Then summing over $j$\n",
    "$$ (a_{11} x_1 y_1 + \\cdots + a_{n1} x_n y_1  ) + (a_{12} x_1 y_2 + \\cdots + a_{n2} x_n y_2) + \\cdots + (a_{1m} x_1 y_m + \\cdots + a_{nm} x_n y_m)  $$\n",
    "   \n",
    "dropping the red and blue to save time in the expansion.\n",
    "   \n",
    "> The expressions *are the same* if the order of summation is reversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitutions\n",
    "---\n",
    "If an expression $y_i = a_{ij}x_j$ is substituted into $B = C_{ij}y_ix_j$ then the dummy index $j$ in the first expression is replaced $y_i = a_{ip}x_p$ to avoid having indices repeated more than twice (see remark 2 above), so then $B=C_{ij}a_{ip} x_p x_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Notation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from matrix algebra that the expression $B=C_{ij}a_{ip} x_p x_j$ does not multiply if converted directly to matrix form, so to put this in matrix form, $$ \\mathbf B_i = (\\mathbf C_{ij} \\mathbf x_j)^{\\mathbf T}( \\mathbf a_{ip} \\mathbf x_p )$$\n",
    "   \n",
    "Matrix notation is different to tensor notation and does not offer an exact replacement because matrix notation requires that a vector is defined as column and transposed to form a row, and that to mulitply two objects (matrix and vector) is an order dependant operation, such that for two non square matrices $\\mathbf A_{mn}$ and $\\mathbf B_{nr}$ the product $\\mathbf C_{mr}= \\mathbf A \\mathbf B $ is possible because $B$ has the same number of rows as $A$ has columns, but $ \\mathbf B \\mathbf A $ is not possible. This is not the case with the summation notation in tensor analysis.\n",
    "\n",
    ">  When Tensor notation is used to denote matrices, the superscript represents the row, and the subscript represents the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker Delta \n",
    "---\n",
    "$$ \\delta^{ij} = \\delta_{ij} = \\delta_j^i = 1 $$ if $i=j$ and zero otherwise\n",
    "\n",
    "\n",
    "   \n",
    "Therefore $\\delta_{11} = 1$, and $\\delta_{12} = 0$, etc\n",
    "   \n",
    "This symbol can be seen to be the tensor notation equivalent to the Identity matrix in matrix algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Symbol\n",
    "---\n",
    "The symbol $e_{ijk...n}$ and $e^{ijk...n}$ with n subscripts or superscripts is called the permutation symbol, and is zero if two indices are identical and equals $(-1)^b$ where $b$ is the number interchanges of consecutive subscripts required to bring $ijk...n$ to the order $123...n$. More simply, in the case of $e_{ijk} = e^{ijk}$ \n",
    " \n",
    " \n",
    "$e_{ijk} =0\\qquad $ if two indices are the same\n",
    "  \n",
    "$e_{ijk} =1\\qquad $ if $i,j,k$ is an even permutation\n",
    "   \n",
    "$e_{ijk} =-1\\qquad $ if $i,j,k$ is an odd permutation\n",
    "\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    "   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain Rule\n",
    "\n",
    "For a function (in this case, a scalar valued function of a vector variable) $w = f(u^k)$ for $k=1,\\cdots n$ and $u^k = u^k(x^j)$ for $j=1,\\cdots m$ Then the rate of change of $w$ with resect to the $x^j$ is expressed as\n",
    "   \n",
    "$$ \\frac{ \\partial w }{\\partial x^j} = \\frac{ \\partial w }{\\partial u^k}\\frac{ \\partial u^k }{\\partial x^j} $$ \n",
    "   \n",
    "In this case $k$ in the term $\\frac{ \\partial w }{\\partial u^k}$ is seen as a <b>subscript </b> and the $k$ in the term\n",
    "$\\frac{ \\partial u^k }{\\partial x^j}$ is seen as a <b>superscript</b> with subscript $j$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot and Cross Product\n",
    "\n",
    "The dot product can be defined as \n",
    "\n",
    "$$ \\mathbf x\\cdot  \\mathbf y = x^i y^i = \\mathbf x \\mathbf y$$\n",
    "\n",
    "Using the permutation symbol to define the basis vectors .. $e_{ijk}$ then $ e_{132} = (-1)^1 = -1$ and $e_{ijk}$ then $ e_{123} = (-1)^0 = 1$ then for each $e_{ijk}$ there is a unique \n",
    "   \n",
    "$$(\\mathbf u \\times \\mathbf v)_i = (e_{ijk}u_j v_k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Coordinates\n",
    "---\n",
    "Using the summation convention, a *coordinate transformation* may be represented as \n",
    "   \n",
    "$$\\bar{x}^i = \\bar{x}^i( x^j )$$ \n",
    "   \n",
    "and\n",
    "   \n",
    "$$x^j = x^j ( \\bar{x}^i  )$$ \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contravariant Tensor\n",
    "   \n",
    "---\n",
    "If $\\mathbf T = (T^i) $ is a vector field (vector function of a vector variable) in the *unbarred* system such that $T^i = T^i(\\mathbf x) = T^i(x^j)$  and it can also be represented in the *barred* system via a coordinate transformation, then  $\\mathbf T$ is a <b>contravariant tensor of rank 1 </b> if the components \n",
    "   \n",
    "$ (T^{ \\color{red}{1}}, T^{ \\color{red}{2}}, \\cdots, T^{ \\color{red}{n}}) $ in the unbarred system\n",
    "   \n",
    "$ (\\bar{T}^{ \\color{blue}{1}},\\bar{T}^{ \\color{blue}{2}},\\cdots, \\bar{T}^{ \\color{blue}{n}}) $ in the barred system\n",
    "   \n",
    "are related by a transformation \n",
    "\n",
    "$$\\large\\bar{T}^{ \\color{blue}{i}} = \\frac{ \\partial \\bar{x}^{ \\color{blue}{i}} }{ \\partial x^{ \\color{red}{j}}} T^{ \\color{red}{j}} $$ \n",
    "   \n",
    "> A contravariant vector has components that change if the coordinates change, but the vector itself does not change. Under an operation like scaling or rotation, the components of a contravariant vector will make a change that cancels the operation. Examples include velocity of a fluid, displacement, acceleration.\n",
    "   \n",
    "> For example, $$\\frac{\\partial y^i}{\\partial t} =\\frac{\\partial y^i}{\\partial x^r}\\frac{\\partial x^r}{\\partial t}$$  \n",
    "> the term $\\frac{\\partial y^i}{\\partial x^r}$ defines the contravariant nature of this term.\n",
    "   \n",
    "with full-on red and blue notation \n",
    "   \n",
    "$$\\large\\color{blue}{\\bar{T}^{ i}} = \\frac{ \\partial \\color{blue}{\\bar{x}^{ i}} }{ \\partial \\color{red}{x^{ j}}} \\color{red}{T^{ j} }$$ \n",
    "   \n",
    "without the red and blue notation\n",
    "\n",
    "$$\\large\\bar{T}^{ i} = \\frac{ \\partial \\bar{x}^{ i} }{ \\partial x^{ j}} T^{ j} $$ \n",
    "   \n",
    "#### So in a *contravariant* tensor the *barred system* that we are converting to appears upstairs in the transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariant Tensor\n",
    "---\n",
    "The vector field $\\mathbf T$ is a covariant tensor is the components in the barred and unbarred coordinate systems obey the following transformation law \n",
    "   \n",
    "$$\\large\\bar{T}_{ \\color{blue}{i}} = \\frac{  \\partial x^{ \\color{red}{j}}  }{\\partial \\bar{x}^{ \\color{blue}{i}} } T_{ \\color{red}{j}} $$ \n",
    "   \n",
    "> A covariant vector has components that change like the coordinates change, The gradient of a scalar function (i.e. the potential) is a covariant tensor, (i.e. $\\mathbf F$ ) , also called a covariant vector.\n",
    "   \n",
    "> For example: $$\\nabla \\mathbf F = \\frac{\\partial F}{\\partial y^i}= \\frac{\\partial F}{\\partial x^p} \\frac{\\partial x^p}{\\partial y^i}$$ the term   $ \\frac{\\partial x^p}{\\partial y^i}$ defines the covariant nature of this. I could equally have use the $y$'s as the unbarred coordinates in both cases, resulting in the opposite appearance. \n",
    "\n",
    "   \n",
    "with the full-on red and blue notation\n",
    "   \n",
    "$$\\large\\color{blue}{\\bar{T}_{ i }} = \\frac{  \\partial \\color{red}{x^{ j }}  }{\\partial \\color{blue}{\\bar{x}^{ i } }} \\color{red}{T_{ j}} $$ \n",
    "   \n",
    "without the red and blue notation\n",
    "\n",
    "$$\\large\\bar{T}_{ i } = \\frac{  \\partial x^{ j }  }{\\partial \\bar{x}^{ i } } T_{ j} $$ \n",
    "   \n",
    "#### So in a *covariant* tensor the coordinate system we are converting to appears *downstairs* in the transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on the Jacobian Matrix\n",
    "> The contravariant tensor transforms with the Jacobian matrix of the system $\\bar{x}^k =\\bar{x}^k (x^j) $ \n",
    "\n",
    ">   \n",
    "> $$ \\bar {T}^i = [\\mathbf J]_j^i T^j $$ \n",
    ">    \n",
    "> where $$[\\mathbf J]_j^i = \\frac{\\partial \\bar{x}^i}{\\partial x^j} $$ \n",
    ">\n",
    "> so, we use the sum over the $j^{\\text{th}}$ column of $[\\mathbf J]$ multiplied by the $j^{\\text{th}}$ component of $T^j$ ... ie\n",
    "   \n",
    "> $\\bar {T}^1 = [\\mathbf J]_j^1 T^j =  \\frac{\\partial \\bar{x}^1}{\\partial x^1}T^1 +\\frac{\\partial \\bar{x}^1}{\\partial x^2}T^2 + \\frac{\\partial \\bar{x}^1}{\\partial x^3}T^3 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> The covariant tensor transforms with the Jacobian matrix of the system $x^j =x^j(\\bar{x}^k ) $ or the reciprocal of the\n",
    "> jacobian matrix of the system $\\bar{x}^k =\\bar{x}^k (x^j) $ \n",
    ">   \n",
    "> $$ \\bar {T}_i = [\\bar{\\mathbf J}] T_j  $$ \n",
    ">    \n",
    "> $\\bar {T}_1 =  \\frac{\\partial x^1}{\\partial \\bar{x}^1}T^1 +\\frac{\\partial x^2}{\\partial \\bar{x}^1}T^2 + \\frac{\\partial x^3}{\\partial \\bar{x}^1}T^3 $\n",
    "\n",
    "Note: Compute the inverse of the Jacobian matrix and compare to the reciprocal jacobian matix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Background \n",
    "\n",
    "For two related systems, \n",
    "   \n",
    "$$\\mathbf x = \\mathbf x(u^1, u^2, u^3)$$\n",
    "\n",
    "and\n",
    "$$\\mathbf u =\\mathbf u(x^1, x^2, x^3) $$ etc\n",
    "\n",
    "Then a point P represented in the cartesian frame, also has a representation in the curvilinear frame $(u^1, u^2, u^3)$.\n",
    "For orthogonal curvilinear coordinates, there are two reciprocal sets of vectors to consider - the tangents to the curves defined by the $u^i$ coordinates and the normals to the surfaces defined by pairs of $u^i$ coordinates, or when a $u^k = \\text{const}$. Let $\\mathbf r = x^1\\mathbf i + x^2\\mathbf j + x^3\\mathbf k$ be a position vector representing the set of all points P in the cartesian system, then the unit tangent vectors are\n",
    "   \n",
    "   \n",
    "$$   \\mathbf e^i= \\frac{\\frac{\\partial \\mathbf r}{\\partial u_i}}{\\left|\\frac{\\partial \\mathbf r}{\\partial u_i} \\right|}=\\frac{1}{h_i}\\frac{\\partial \\mathbf r}{\\partial u_i}   $$\n",
    "\n",
    "where the $h_i=\\left|\\frac{\\partial r}{\\partial u^i}\\right|$ are the scale factors or Lame coefficients and the $\\mathbf e^i$ are in the directions of the tangents of $u^i$ curves. \n",
    "   \n",
    "The reciprocal set of vectors is \n",
    "   \n",
    "$$ \\hat{\\mathbf e}^p = \\frac{\\nabla u^p}{\\left|\\nabla u^p\\right|}$$ \n",
    "   \n",
    "Where $\\nabla u^1 = \\frac{\\partial u^1}{\\partial x^1}\\mathbf i +\\frac{\\partial u^1}{\\partial x^2}\\mathbf j +\\frac{\\partial u^1}{\\partial x^3}\\mathbf k$. These vectors are the normals to the surface such that $\\nabla u^p$ is normal to $u^p = \\text{const}$. \n",
    "\n",
    "The terms $\\frac{\\partial \\mathbf r}{\\partial u_i}$ and $\\nabla u^p$ are called unitary base vectors , but are not unit vectors. If a vector $\\mathbf v$ is represented in these two systems...\n",
    "   \n",
    "$$ \\mathbf v = a^i \\frac{\\partial \\mathbf r}{\\partial u^i} $$\n",
    "and\n",
    "$$ \\mathbf v = b^p \\nabla u^p $$\n",
    "\n",
    "Then the $a^i$ are the <b>contravariant</b> components of $\\mathbf v$ and the $b^p$ are the <b>covariant</b> components - they are only coincident / identical if the curvilinear system is orthogonal. This case will be most systems, however these definitions are useful for the concept of the metric.\n",
    "\n",
    "Arc length : $(ds)^2 = (h_1)^2 (du^1)^2+(h_1)^2 (du^1)^2+(h_3)^2 (du^3)^2 = (h_p)^2 (du^p)^2$\n",
    "   \n",
    "Volume: $dV = h_1\\ h_2\\ h_3\\ du^1 du^2 du^3 = |\\mathbf e^1 \\cdot\\mathbf e^2 \\times \\mathbf e^3|$ (scalar triple product).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invariants\n",
    "Any quantity that does not change with a change of basis is called an invariant. For example, a scalar function of a vector variable, a potential $\\phi(x^i)$ is transformed to the barred coordinate system and still has the same value, therefore $\\phi$ is said to be <b>invariant</b> or a <b>tensor of rank zero</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Order Tensors\n",
    "---\n",
    "If there are $n^2$ quantities defining a matrix field, or matrix of scalar fields $\\mathbf T = T^{ij}(\\mathbf x) $ with components in both the barred and unbarred coordinate systems\n",
    "    \n",
    "#### Contravariant\n",
    "      \n",
    "$$\\large\\bar{T}^{ i r} = \\frac{ \\partial \\bar{x}^{ i} }{ \\partial \\smash{x^{ j}}} \\frac{ \\partial \\bar{x}^{ r} }{ \\partial x^{ s}} T^{ j s} $$ \n",
    "\n",
    "#### Covariant\n",
    "   \n",
    "$$\\large\\bar{T}_{ i r } = \\frac{  \\partial x^{ j }  }{\\partial \\smash{\\bar{x}^{ i } }} \\frac{  \\partial x^{ s }  }{\\partial \\bar{x}^{ r } }T_{ j s} $$ \n",
    "\n",
    "#### Mixed\n",
    "   \n",
    "$$\\large\\bar{T}^i_r = \\frac{ \\partial \\bar{x}^{ i} }{ \\partial \\smash{x^{ j}}} \\frac{  \\partial x^{ s }  }{\\partial \\bar{x}^{ r } }T^j_s $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> An intutive way to think about this is that a zero order tensor is a scalar, a first order tensor is a vector, and a  second order tensor is a matrix ... for example the mixed tensor written above ( in 3 dimensions ) would appear as the following\n",
    "\n",
    "$$ \\bar{T}^i_r =\\left( \\begin{matrix} \\bar{T}^1_1 &\\bar{T}^1_2 & \\bar{T}^1_3 \\\\ \\bar{T}^2_1&\\bar{T}^2_2&\\bar{T}^2_3 \\\\ \\bar{T}^3_1&\\bar{T}^3_2&\\bar{T}^3_3  \\end{matrix}\\right) $$\n",
    "   \n",
    "and\n",
    "   \n",
    "$$\\frac{ \\partial \\bar{x}^{ i} }{ \\partial \\smash{x^{ j}}}= \\left(\\begin{matrix} \\frac{\\partial \\bar{x}^1}{\\partial x^1} &\\frac{\\partial \\bar{x}^1}{\\partial x^2} & \\frac{\\partial \\bar{x}^1}{\\partial x^3} \\\\\\ \\frac{\\partial \\bar{x}^2}{\\partial x^1} & \\frac{\\partial \\bar{x}^2}{\\partial x^2} & \\frac{\\partial \\bar{x}^2}{\\partial x^3} \\\\\\ \\frac{\\partial \\bar{x}^3}{\\partial x^1} & \\frac{\\partial \\bar{x}^3}{\\partial x^2} &\\frac{\\partial \\bar{x}^3}{\\partial x^3}  \\end{matrix}\\right)$$\n",
    "   \n",
    "and\n",
    "   \n",
    "$$\\frac{  \\partial x^{ s }  }{\\partial \\bar{x}^{ r } }=\\left( \\begin{matrix} \\frac{\\partial x^1}{\\partial \\bar{x}^1} &\\frac{\\partial x^1}{\\partial \\bar{x}^2} & \\frac{\\partial x^1}{\\partial \\bar{x}^3} \\\\\\ \\frac{\\partial x^2}{\\partial \\bar{x}^1} & \\frac{\\partial x^2}{\\partial \\bar{x}^2} & \\frac{\\partial x^2}{\\partial \\bar{x}^3} \\\\\\ \\frac{\\partial x^3}{\\partial \\bar{x}^1} & \\frac{\\partial x^3}{\\partial \\bar{x}^2} &\\frac{\\partial x^3}{\\partial \\bar{x}^3}  \\end{matrix}\\right) $$\n",
    "\n",
    "and $$\n",
    "T^j_s =\\left( \\begin{matrix} T^1_1 &T^1_2 & T^1_3 \\\\\\ T^2_1&T^2_2&T^2_3 \\\\\\ T^3_1&T^3_2&T^3_3  \\end{matrix}\\right) $$\n",
    "   \n",
    "and so clearly expanding up the full equation is a tedious and laborious process.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Order Tensors\n",
    "   \n",
    "For a generalized vector field $\\mathbf T = T^{i_1,\\cdots i_p}_{r_1,\\cdots r_q}$ consisting of $(p+q)$ scalar fields, and is of rank $p+q$, contravariant off order p and covariant of order q defined in both coordinate systems, then if the components of the matrix field transform \n",
    "   \n",
    "   $$ \\large\\bar{T}^{i_1,\\cdots i_p}_{r_1,\\cdots r_q} =  T^{j_1,\\cdots j_p}_{s_1,\\cdots s_q} \\frac{ \\partial \\bar{x}^{ i_1} }{ \\partial \\smash{x^{ j_1}} }\\frac{ \\partial \\bar{x}^{ i_2} }{ \\partial \\smash{x^{ j_2}}} ... \\frac{ \\partial \\bar{x}^{ i_p} }{ \\partial \\smash{x^{ j_p}}} \\frac{ \\partial x^{ s_1} }{ \\partial \\bar{x}^{ r_1}} ... \\frac{ \\partial x^{ s_q} }{ \\partial \\bar{x}^{ r_q}} $$\n",
    "   \n",
    "  This is a generalized mixed tensor of rank $p+q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Addition\n",
    "\n",
    "The sum of two tensors of the same rank and type also produce a tensor of the same rank and type . e.g  \n",
    "   \n",
    "   $$\\large T^{pq}_{r} = A^{pq}_{r} + B^{pq}_{r}$$. \n",
    "   \n",
    "This operation is associative and commutative. The same is true for subtraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Product\n",
    "   \n",
    "The outer product of two tensors is a tensor\n",
    "      \n",
    "$$\\large U^{i_1\\cdots i_ak_1\\cdots k_c}_{j_1\\cdots j_bl_d\\cdots l_d}= \\left(S^{i_1\\cdots i_a}_{j_1\\cdots j_b}\\cdot T^{k_1\\cdots k_c}_{l_d\\cdots l_d} \\right)$$\n",
    "   \n",
    "of order $a + b + c + d$, and is covariant of order $b+d$ and contravariant of order $a+c$. The outer product is commutative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contraction\n",
    "   \n",
    "A contraction is defined by setting two indices equal for example setting some chosen index in the range $(i_1\\cdots i_a)$ equal to some index in the range $r_1\\cdots r_b)$, then the contraction of \n",
    "   \n",
    "$$\\large T^{i_1\\cdots i_a}_{r_1\\cdots r_b}$$ \n",
    "   \n",
    "is a tensor of rank $(a+b-2)$, covariant of order $b-1$ and contravariant of order $a-1$.\n",
    "\n",
    "This operation uses the fact that $\\frac{\\partial x^p}{\\partial x^q} = \\delta^p_q$ as in calculus, $\\frac{dx}{dx} = 1$, but the change in the x axis with respect to the z axis is zero, since they are orthogonal.\n",
    "\n",
    "> A contraction can result in a loss of information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The contraction of a second order tensor $T^i_j$ would be $T^i_i=T^1_1+T^2_2+T^3_3$ and is the trace of the tensor in matrix form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Product\n",
    "    \n",
    "This operation is equivalent to an outer product followed by a contraction.\n",
    "   \n",
    "If $T^i$ is a contravariant Tensor of rank 1 and $S_{ji}$ is covariant of rank 2 then since $T^i = \\bar{ T^j } \\frac{\\partial x^i}{\\partial \\bar{x}^j}$ and $S_{ji}= \\bar{S}_{pq}\\frac{\\partial \\bar{x}^p}{\\partial x^j}\\frac{\\partial \\bar{x}^q}{\\partial x^i} $ \n",
    "\n",
    "$$ (TS)_j = (T^i)(S_{ji}) = \\left(\\bar{ T^r } \\frac{\\partial x^i}{\\partial \\bar{x}^r}\\right) \\left(\\bar{S}_{pq}\\frac{\\partial \\bar{x}^p}{\\partial x^j}\\frac{\\partial \\bar{x}^q}{\\partial x^i} \\right) = \\color{red}{\\frac{\\partial x^i}{\\partial \\bar{x}^r}}\\color{blue}{ \\frac{\\partial \\bar{x}^q}{\\partial x^i} }\\color{black}{\\frac{\\partial \\bar{x}^p}{\\partial x^j} \\bar{T}^r \\bar{S}_{pq}} = \\delta_{\\color{red}{r}}^{\\color{blue}{q}}\\color{black}{\\frac{\\partial \\bar{x}^p}{\\partial x^j}} \\color{black}{\\bar{T}}^{\\color{red}{r}} \\color{black}{S}_{p\\color{blue}{q}} = \\color{black}{\\frac{\\partial \\bar{x}^p}{\\partial x^j} (\\bar{T S})_{p} }$$\n",
    "   \n",
    "and the result is a covariant vector. The result of multipliying the product $\\frac{\\partial x^i}{\\partial \\bar{x}^r}\\frac{\\partial \\bar{x}^q}{\\partial x^i} = \\delta^q_r$ since the $x^i$'s cancel and the remaining differential quotient \n",
    "$\\frac{\\partial \\bar{x}^q}{\\partial \\bar{x}^r} = \\delta^q_r$ since this is equivalent to the change in x with respect to y when x is not a function of y when $r\\neq q$ and so those terms are zero, and when $r=q$ the terms are 1. The $\\delta_r^q$ term remaining then multiplies with the $\\bar{T}^r \\bar{S}_{pq}$ terms, effectively leaving the sum over p, since the other terms are zero. This second part is equivalent to a contraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Tensor\n",
    "\n",
    "The metric Tensor is defined as $\\mathbf g =  g_{ij}$ and has the following properties...\n",
    "   \n",
    "1) $\\mathbf g$ is positive definite (and also the inverse $\\mathbf g^{-1}$\n",
    "   \n",
    "2) non-singular $|g_{ij}|\\neq 0$\n",
    "   \n",
    "3) symmetric\n",
    "   \n",
    "4) has 2nd order partial derivatives (class $C^2$\n",
    "   \n",
    "5) Is invariant with change of coordinates (see arc length below).\n",
    "   \n",
    "The concept of distance in Euclidean space, for any generalized cartesian, orthogonal curvilinear, affine coordinates is:\n",
    "   \n",
    "$$ (ds)^2 = g_{ij} (dx^i)(dx^j) $$\n",
    "\n",
    "where for Euclidean systems $g_{ij}=0$ if $i\\neq j $\n",
    "\n",
    "For a coordinate system in the Euclidean metric, \n",
    "\n",
    "$$ \\mathbf G = \\mathbf J^T \\mathbf J $$\n",
    "   \n",
    "and for example, the orthogonal curvlinear coordinate systems (spherical, cylindrical, dual paraboloidal, elliptic) \n",
    "all have scale factors (Lame Coefficients) and are coordinate systems of the Euclidean metric, these scale factors appear as the square roots of the diagonal elements of the metric.\n",
    "\n",
    "#### definition: The metric tensor is a covariant tensor of 2nd order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raising and lowering indices\n",
    "   \n",
    "#### definition. The inner product of a contravariant vector and a covariant tensor of 2nd order is a covariant vector. Therefore\n",
    "   \n",
    "$$T_i = g_{ij} T^j $$\n",
    "\n",
    "*defines* the operation of lowering indices of a contravariant vector. Equivalently \n",
    "   \n",
    "$$ T^i = g^{ij}T_j$$\n",
    "\n",
    "where $g^{ij} = (g_{ij})^{-1}$ is the inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$ (ds)^2 = (dx^1)^2+(dx^2)^2+(dx^3)^2$$\n",
    "\n",
    "in the context of a change of variables, the metric tensor is used - but to change between two coordinate systems\n",
    "the cartesian $x^r=x^r(u^i)$ to the $u^i$ system the scale factors are used that relate cartesian to curvilinear, they are elements of the Jacobian matrix that relate one system to the other. For example the scale factors for the $u^i$ system might be $(h_1,h_2,h_3)$, then the *line element* becomes\n",
    "   \n",
    "$$ (ds)^2 = (h_1)^2(du^1)^2+(h_2)^2(du^2)^2+(h_3)^2(du^3)^2$$\n",
    "\n",
    "*because* the change in the variables $dx^i$ with respect to the $u^k$ system requires the Jacobian, and if for example the $u^k$ system was a Euclidean metric the variables can be derived easily\n",
    "   \n",
    "$$\\frac{dx^1}{du^1} = h_1, \\implies dx^1 = h_1 du^1 $$ \n",
    "   \n",
    "etc, then the line element is easily found through substitution. The general form of $dx^i$ in $n$ dimensions is \n",
    "   \n",
    "$$ dx^i = \\frac{dx^i}{du^j}du^j = \\frac{dx^i}{du^1}du^1 +\\frac{dx^i}{du^2}du^2+\\cdots \\frac{dx^i}{du^n}du^n $$\n",
    "   \n",
    "and so the scale factor becomes quite different (and the metric).\n",
    "   \n",
    "The line element is important, as it ensures that distance between two points is an invariant between coordinate systems. This is why it is often attached to the notion of the metric. The metric can also be associated with the concept of a dot product between two vectors defined on a surface transforming as $\\mathbf a^T \\mathbf G \\mathbf a$ and \n",
    "from the dot product the angle can be produced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
